{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e61245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "anno_file = 'hicodet/instances_train2015.json'\n",
    "with open(anno_file, 'r') as f:\n",
    "    anno = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae846420",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = list(range(len(anno['filenames'])))\n",
    "for empty_idx in anno['empty']:\n",
    "    idx.remove(empty_idx)\n",
    "random.shuffle(idx)\n",
    "# num_anno = [0 for _ in range(self.num_interation_cls)]\n",
    "# for anno in f['annotation']:\n",
    "#     for hoi in anno['hoi']:\n",
    "#         num_anno[hoi] += 1\n",
    "\n",
    "_idx = idx\n",
    "# _num_anno = num_anno\n",
    "\n",
    "_anno = anno['annotation']\n",
    "_filenames = anno['filenames']\n",
    "_image_sizes = anno['size']\n",
    "_class_corr = anno['correspondence']\n",
    "_empty_idx = anno['empty']\n",
    "_objects = anno['objects']\n",
    "_verbs = anno['verbs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5a4e1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_lens = 4\n",
    "all_list = []\n",
    "already_sampled = [0 for i in range(600)]\n",
    "for k in idx:\n",
    "    \n",
    "    anno_one = _anno[k]\n",
    "    filename = _filenames[k]\n",
    "    size = _image_sizes[k]\n",
    "    \n",
    "    hois = anno_one['hoi']\n",
    "    objects = anno_one['object']\n",
    "    verbs = anno_one['verb']\n",
    "    boxes_h = anno_one['boxes_h']\n",
    "    boxes_o = anno_one['boxes_o']\n",
    "    \n",
    "    \n",
    "    all_boxes_h = []\n",
    "    all_boxes_o = []\n",
    "    all_hois = []\n",
    "    all_objects = []\n",
    "    all_verbs = []\n",
    "    pair_with_original = []\n",
    "    add_true = False\n",
    "    for i,hoi in enumerate(hois):\n",
    "        if already_sampled[hoi] != sample_lens:\n",
    "            already_sampled[hoi] +=1\n",
    "            all_boxes_h.append(boxes_h[i])\n",
    "            all_boxes_o.append(boxes_o[i])\n",
    "            all_hois.append(hoi)\n",
    "            all_objects.append(objects[i])\n",
    "            all_verbs.append(verbs[i])\n",
    "            pair_with_original.append(i)\n",
    "            add_true = True\n",
    "        else:\n",
    "            pass\n",
    "    if add_true:\n",
    "        all_list.append(dict(\n",
    "            original_idx=k,\n",
    "            boxes_h=all_boxes_h,\n",
    "            boxes_o=all_boxes_o,\n",
    "            hoi=all_hois,\n",
    "            object=all_objects,\n",
    "            verb=all_verbs,\n",
    "            pair_with_original=pair_with_original,\n",
    "            filename=filename,\n",
    "            size=size))\n",
    "    if sum(already_sampled) == sample_lens * 600:break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "579ecedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11360"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx[119]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "79f6e4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(_anno[100]['boxes_h'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6f00f0b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (1605560111.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_36962/1605560111.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    int(_anno[100]['boxes_h'][0]\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "int(_anno[100]['boxes_h'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "05ce5b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[270.0, 12.0, 638.0, 471.0] in _anno[119]['boxes_h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b4f1e6d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_filenames.index('HICO_train2015_00000120.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa39d0a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('few_shot_pickle/few_shot_{}_{}.p'.format(sample_lens,sum(already_sampled)),'wb') as f:\n",
    "    pickle.dump(all_list,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9912e126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1464, 2189)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_list\n",
    "   ),sum(already_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "898f4f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = 'union_embeddings_cachemodel_crop_padding_zeros_vitb16.p'\n",
    "anno_vit16 = pickle.load(open(file1,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5678899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_vit16 = list(anno_vit16.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "150c27e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7218"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4bed4184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_idx.index(7218)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3f1ea361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes_h': [[310.0, 248.0, 325.0, 277.0],\n",
       "  [322.0, 231.0, 335.0, 278.0],\n",
       "  [414.0, 222.0, 439.0, 288.0],\n",
       "  [109.0, 232.0, 138.0, 318.0],\n",
       "  [439.0, 209.0, 467.0, 304.0],\n",
       "  [1.0, 189.0, 23.0, 313.0],\n",
       "  [567.0, 192.0, 617.0, 309.0],\n",
       "  [328.0, 265.0, 394.0, 392.0]],\n",
       " 'boxes_o': [[368.0, 119.0, 378.0, 131.0],\n",
       "  [368.0, 119.0, 378.0, 131.0],\n",
       "  [368.0, 119.0, 378.0, 131.0],\n",
       "  [368.0, 119.0, 378.0, 131.0],\n",
       "  [368.0, 119.0, 378.0, 131.0],\n",
       "  [368.0, 119.0, 378.0, 131.0],\n",
       "  [368.0, 119.0, 378.0, 131.0],\n",
       "  [368.0, 119.0, 378.0, 131.0]],\n",
       " 'hoi': [501, 501, 501, 501, 501, 501, 501, 501],\n",
       " 'object': [32, 32, 32, 32, 32, 32, 32, 32],\n",
       " 'verb': [57, 57, 57, 57, 57, 57, 57, 57]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_anno[7218]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "89457864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes_h': array([[108.1 , 115.25, 113.75, 129.2 ],\n",
       "        [112.4 , 107.3 , 117.25, 129.8 ],\n",
       "        [144.5 , 103.1 , 153.6 , 134.4 ],\n",
       "        [ 37.8 , 107.8 ,  48.3 , 148.4 ],\n",
       "        [153.2 ,  97.06, 163.5 , 141.9 ],\n",
       "        [  0.  ,  87.75,   8.05, 146.1 ],\n",
       "        [198.1 ,  89.1 , 216.  , 144.2 ],\n",
       "        [114.44, 123.2 , 137.9 , 182.9 ]], dtype=float16),\n",
       " 'boxes_o': array([[128.5 ,  55.06, 132.2 ,  61.12],\n",
       "        [128.5 ,  55.06, 132.2 ,  61.12],\n",
       "        [128.5 ,  55.06, 132.2 ,  61.12],\n",
       "        [128.5 ,  55.06, 132.2 ,  61.12],\n",
       "        [128.5 ,  55.06, 132.2 ,  61.12],\n",
       "        [128.5 ,  55.06, 132.2 ,  61.12],\n",
       "        [128.5 ,  55.06, 132.2 ,  61.12],\n",
       "        [128.5 ,  55.06, 132.2 ,  61.12]], dtype=float16),\n",
       " 'verbs': array([57, 57, 57, 57, 57, 57, 57, 57]),\n",
       " 'hois': array([501, 501, 501, 501, 501, 501, 501, 501]),\n",
       " 'union_boxes': array([[108.1 ,  55.06, 132.2 , 129.2 ],\n",
       "        [112.4 ,  55.06, 132.2 , 129.8 ],\n",
       "        [128.5 ,  55.06, 153.6 , 134.4 ],\n",
       "        [ 37.8 ,  55.06, 132.2 , 148.4 ],\n",
       "        [128.5 ,  55.06, 163.5 , 141.9 ],\n",
       "        [  0.  ,  55.06, 132.2 , 146.1 ],\n",
       "        [128.5 ,  55.06, 216.  , 144.2 ],\n",
       "        [114.44,  55.06, 137.9 , 182.9 ]], dtype=float16),\n",
       " 'union_features': array([[ 0.5354473 , -0.69323593, -0.53489506, ...,  0.2642452 ,\n",
       "          0.24269152,  0.35190213],\n",
       "        [ 0.5262514 , -0.9220668 , -0.45726022, ...,  0.20075558,\n",
       "          0.07857453,  0.4158893 ],\n",
       "        [ 0.2874    , -0.35995793, -0.25610718, ...,  0.00733804,\n",
       "          0.16898578,  0.05768827],\n",
       "        ...,\n",
       "        [ 0.2564357 , -0.66164094,  0.18959744, ...,  0.37229055,\n",
       "          0.25652063,  0.8040758 ],\n",
       "        [ 0.33284748, -0.9466456 , -0.04367064, ...,  0.22148186,\n",
       "          0.2694062 ,  0.4105407 ],\n",
       "        [ 0.5280305 , -0.927773  , -0.46527025, ..., -0.05525293,\n",
       "         -0.06458287,  0.32153836]], dtype=float32),\n",
       " 'huamn_features': array([[ 1.0814455 , -0.80763286, -0.43853045, ...,  0.13105327,\n",
       "          0.20294625, -0.00440049],\n",
       "        [ 0.24283653, -0.7290147 , -0.07999641, ...,  0.05587354,\n",
       "          0.24019335,  0.16873252],\n",
       "        [ 0.30902475, -0.09923854, -0.37002045, ..., -0.01634924,\n",
       "          0.40985432,  0.06939629],\n",
       "        ...,\n",
       "        [ 0.69821906, -0.83399343, -0.1538696 , ..., -0.19841829,\n",
       "          0.33855066,  0.4078727 ],\n",
       "        [ 0.5650844 , -0.8164952 , -0.08650105, ..., -0.29033813,\n",
       "          0.21786852,  0.30611986],\n",
       "        [ 0.5829817 , -0.6828526 , -0.4407058 , ..., -0.04841211,\n",
       "         -0.1315903 ,  0.18858467]], dtype=float32),\n",
       " 'object_features': array([[ 0.8200095 , -0.2662376 ,  0.06384112, ...,  0.01342161,\n",
       "          0.10436542, -0.14002253],\n",
       "        [ 0.8200095 , -0.2662376 ,  0.06384112, ...,  0.01342161,\n",
       "          0.10436542, -0.14002253],\n",
       "        [ 0.8200095 , -0.2662376 ,  0.06384112, ...,  0.01342161,\n",
       "          0.10436542, -0.14002253],\n",
       "        ...,\n",
       "        [ 0.8200095 , -0.2662376 ,  0.06384112, ...,  0.01342161,\n",
       "          0.10436542, -0.14002253],\n",
       "        [ 0.8200095 , -0.2662376 ,  0.06384112, ...,  0.01342161,\n",
       "          0.10436542, -0.14002253],\n",
       "        [ 0.8200095 , -0.2662376 ,  0.06384112, ...,  0.01342161,\n",
       "          0.10436542, -0.14002253]], dtype=float32),\n",
       " 'objects': array([32, 32, 32, 32, 32, 32, 32, 32]),\n",
       " 'global_feature': array([ 7.47563541e-01, -1.28535831e+00, -1.42868876e-01,  1.50605351e-01,\n",
       "        -3.51949871e-01, -2.85906732e-01,  2.35539317e-01, -3.67240496e-02,\n",
       "         2.62601405e-01, -3.21592033e-01,  5.08033752e-01, -9.58437473e-02,\n",
       "        -1.25267893e-01,  3.20124626e-02, -3.38482767e-01,  1.00384362e-01,\n",
       "         1.29892856e-01, -5.19460589e-02, -1.08487055e-01, -3.03585291e-01,\n",
       "         1.00778356e-01,  1.43304756e-02,  2.58374419e-02, -1.76631231e-02,\n",
       "        -6.25721514e-02,  2.73618788e-01, -9.02432799e-02,  3.80028129e-01,\n",
       "         2.48227338e-03,  1.31989628e-01, -1.32043645e-01, -1.76431641e-01,\n",
       "        -4.95308414e-02, -2.86666781e-01, -1.35318637e+00, -2.13288665e-01,\n",
       "         4.81209755e-01,  1.07219599e-01,  9.42310020e-02, -2.62422264e-01,\n",
       "         6.70327187e-01,  2.54563689e-01,  1.85441703e-01,  5.20531178e-01,\n",
       "         1.08449616e-01,  1.10287912e-01,  2.57847738e-03, -2.41882391e-02,\n",
       "         2.10603043e-01, -4.48393822e-01, -2.09028172e+00,  3.13243680e-02,\n",
       "        -2.17623319e-02,  3.45222563e-01, -5.63473329e-02,  1.65548891e-01,\n",
       "         6.21752553e-02, -5.51019721e-02,  4.62888405e-02, -1.87803641e-01,\n",
       "         5.70073426e-01,  6.03373051e-02,  3.53294387e-02, -1.05289174e-02,\n",
       "        -6.30910769e-02, -1.41728804e-01,  7.50901327e-02, -2.04082131e-01,\n",
       "        -1.06791116e-01,  2.06614122e-01,  3.20854902e-01,  4.31134880e-01,\n",
       "         1.22761793e-01, -3.97238731e-01,  8.12434733e-01,  1.68414563e-01,\n",
       "         8.99297595e-02, -3.67750078e-01,  4.71394993e-02, -2.10787892e-01,\n",
       "        -8.22615847e-02, -3.41766149e-01,  1.38427287e-01, -2.25513905e-01,\n",
       "        -2.81402826e-01,  2.70772636e-01, -9.82832074e-01,  3.70217055e-01,\n",
       "        -2.17468515e-01, -1.72511920e-01, -2.97834933e-01,  3.08058709e-01,\n",
       "         1.86224595e-01, -1.80908442e-01,  3.19455057e-01, -2.27178007e-01,\n",
       "         3.40866685e-01,  1.49432048e-01,  9.90481451e-02,  5.80703557e-01,\n",
       "         1.64021291e-02,  2.51822490e-02,  1.30672574e-01,  5.99072099e-01,\n",
       "        -2.56351888e-01,  3.20906267e-02,  2.40077630e-01,  1.07120104e-01,\n",
       "        -4.38349545e-01,  3.66800100e-01,  1.80554166e-01, -3.91735524e-01,\n",
       "        -1.18518434e-01, -2.28655189e-01,  3.80391926e-01, -3.26622352e-02,\n",
       "        -1.79173872e-01, -2.00118706e-01,  3.25974762e-01,  1.55894486e-02,\n",
       "         5.33233546e-02,  2.67563701e-01,  1.86862662e-01,  1.55297291e+00,\n",
       "         8.31344962e-01, -1.74648762e-01, -3.92153233e-01,  1.99951231e-01,\n",
       "         2.63448745e-01, -8.78786445e-02, -3.87919173e-02, -6.25566766e-02,\n",
       "         4.92572300e-02, -3.71086925e-01,  9.79485586e-02,  2.20200419e-01,\n",
       "        -2.82458931e-01,  2.01395094e-01, -2.53845572e-01,  7.12688714e-02,\n",
       "         4.60975394e-02,  2.08597973e-01, -1.96562886e-01,  3.73464733e-01,\n",
       "         3.56917232e-01, -2.23841757e-01,  6.37548387e-01, -3.23740155e-01,\n",
       "         1.63630277e-01, -4.80218828e-02,  6.92051947e-02, -2.18306333e-01,\n",
       "         2.00508177e-01, -3.06950808e-01,  8.33019391e-02,  4.01612431e-01,\n",
       "         2.15649575e-01,  5.00116885e-01,  2.18021557e-01, -1.71836823e-01,\n",
       "         3.06070328e-01, -8.60917196e-02, -5.56477308e-01, -2.22379386e-01,\n",
       "        -2.93068528e-01, -2.06301317e-01, -3.23332101e-01, -2.86938757e-01,\n",
       "        -7.54289925e-01,  1.73535421e-01, -3.75872761e-01,  1.81036949e-01,\n",
       "         2.40801558e-01,  4.20903593e-01,  5.98543227e-01,  3.06036621e-01,\n",
       "        -5.50535262e-01,  9.90470946e-02, -3.27316850e-01,  2.88697034e-02,\n",
       "        -1.19400822e-01, -2.41053656e-01,  6.42146990e-02, -9.54855859e-01,\n",
       "         1.41766593e-02,  5.95383234e-02, -2.30172381e-01,  1.40894428e-01,\n",
       "        -6.66268356e-03,  4.54838991e-01, -5.28763950e-01, -2.55473167e-01,\n",
       "         3.73427987e-01,  1.09201171e-01, -5.13351858e-01, -3.46911460e-01,\n",
       "        -3.22039038e-01, -3.01212162e-01, -3.88818681e-01,  2.69331068e-01,\n",
       "         4.39425977e-03,  1.72047000e-02,  7.12474883e-02,  3.11849684e-01,\n",
       "        -4.35937136e-01,  3.14473122e-01, -2.05508888e-01, -4.11255956e-01,\n",
       "         6.07583582e-01, -4.42532212e-01, -3.74854356e-01,  4.91394699e-01,\n",
       "        -1.54078841e-01,  3.93204600e-01,  1.51308909e-01, -1.30016431e-01,\n",
       "        -1.78352997e-01,  3.46088260e-01,  2.26953402e-01,  1.90805793e-01,\n",
       "         3.13905448e-01, -3.96086186e-01,  9.47586298e-02, -2.31537726e-02,\n",
       "        -9.57360864e-02,  4.21581194e-02,  5.05900919e-01,  7.75416121e-02,\n",
       "         4.41506475e-01,  3.99506748e-01, -1.79450914e-01,  1.68528184e-01,\n",
       "         3.32141131e-01, -2.88373321e-01, -1.08497925e-01, -2.87112504e-01,\n",
       "         1.94419906e-01, -3.39694060e-02, -1.70914456e-01, -1.40477389e-01,\n",
       "         1.78926259e-01, -4.74417478e-01,  3.54086071e-01, -2.83489563e-02,\n",
       "        -8.55202675e-01, -1.19255096e-01, -3.14922214e-01,  1.07541092e-01,\n",
       "         2.67904960e-02, -2.93145508e-01,  4.50757086e-01, -1.17803228e+00,\n",
       "         6.78697824e-01,  2.24310443e-01, -9.61993396e-01,  1.32385835e-01,\n",
       "        -8.28732848e-01, -8.50760415e-02,  3.14335041e-02, -7.14477152e-02,\n",
       "        -3.84532601e-01, -2.78580021e-02,  2.91161835e-01, -1.14937589e-01,\n",
       "         4.42288369e-01, -1.41504121e+00, -9.19141844e-02,  2.59315103e-01,\n",
       "         2.23845109e-01, -2.44814664e-01,  2.48482496e-01, -1.34204313e-01,\n",
       "         1.43165857e-01, -1.40227556e-01,  4.28968251e-01, -7.62018740e-01,\n",
       "         1.08898178e-01, -3.80211361e-02,  2.98603147e-01, -1.00223221e-01,\n",
       "         3.50048065e-01, -1.98927581e-01, -1.73179090e-01, -6.52992576e-02,\n",
       "         1.44469664e-01,  9.50505733e-02,  5.70458882e-02, -3.26574415e-01,\n",
       "        -1.74160656e-02, -1.77695900e-01,  2.01504767e-01, -5.17031290e-02,\n",
       "        -3.23089927e-01,  2.24120393e-02,  2.94786274e-01,  2.43850350e-01,\n",
       "        -5.31619728e-01,  5.19561954e-02, -4.85398144e-01, -2.91715652e-01,\n",
       "        -4.83682394e-01,  2.06591576e-01,  2.35118836e-01,  1.15098625e-01,\n",
       "        -9.66112781e-03, -7.03395844e-01,  3.25147957e-01,  5.56361914e-01,\n",
       "        -3.18679251e-02,  3.14244121e-01,  4.56455469e-01,  2.34456703e-01,\n",
       "        -3.14951211e-01, -8.12428892e-02, -1.53284594e-01,  1.12377495e-01,\n",
       "        -2.94097722e-01, -9.97745991e-02,  3.15627098e-01,  1.70696661e-01,\n",
       "        -1.89493611e-01,  2.63825744e-01, -4.02235657e-01,  1.17940955e-01,\n",
       "         6.74942182e-03, -4.80488352e-02,  6.85975328e-02, -9.97643545e-02,\n",
       "        -3.66154090e-02,  4.79267351e-03,  3.94267142e-02, -1.54901966e-01,\n",
       "        -1.33329735e-03, -2.75938362e-01, -4.67879504e-01, -8.78094062e-02,\n",
       "         1.02178268e-01, -1.21981159e-01,  2.44967900e-02,  2.06646919e-01,\n",
       "         2.60129794e-02, -4.42800701e-01,  8.99086475e-01, -1.94446504e-01,\n",
       "         3.30713093e-01, -2.19152242e-01,  1.39678910e-01,  1.28755262e-02,\n",
       "         2.11481139e-01, -4.47746068e-02, -4.55429167e-01,  3.65764767e-01,\n",
       "        -1.63121119e-01,  1.56889886e-01, -8.99805054e-02, -3.84754986e-01,\n",
       "        -9.54930186e-02, -1.54185221e-01,  1.44699708e-01, -1.19475067e-01,\n",
       "         4.26134393e-02, -2.21956924e-01,  1.05090380e+00, -3.43787670e-01,\n",
       "         1.85127214e-01,  3.88343818e-02,  1.96877971e-01,  5.74323893e-01,\n",
       "         3.50509631e-03, -7.12100789e-02, -1.31111085e-01, -2.72135884e-01,\n",
       "        -2.53463537e-02, -7.14491904e-02, -2.98291096e-03, -3.12611423e-02,\n",
       "         5.76605797e-01, -3.64955217e-01, -8.60478878e-02,  2.59533793e-01,\n",
       "        -8.80575180e-02,  2.05608755e-02, -2.30724767e-01, -4.74876195e-01,\n",
       "        -4.56181794e-01,  2.03942478e-01,  5.30709289e-02, -1.05539367e-01,\n",
       "         1.20964065e-01, -1.29771933e-01, -8.92233551e-01,  3.42706203e-01,\n",
       "         6.42760843e-02,  1.52426558e-02,  4.01965648e-01, -1.63061038e-01,\n",
       "        -2.00889915e-01, -1.29865766e-01,  2.13400006e-01,  9.60537195e-02,\n",
       "        -1.93960994e-01, -1.00398459e-01,  7.53211901e-02,  3.93944710e-01,\n",
       "         4.40919846e-01, -1.87655598e-01, -7.17624351e-02,  1.66299031e-03,\n",
       "        -1.44153804e-01, -2.91477703e-02,  3.79764326e-02,  8.40309381e-01,\n",
       "         2.50205994e-01, -5.50189801e-02,  2.97074690e-02, -1.77681667e-03,\n",
       "        -1.23177208e-01,  4.35924202e-01, -1.62654407e-02,  2.09556501e-02,\n",
       "         5.83523028e-02, -9.18398649e-02,  3.91560555e-01,  8.07307363e-02,\n",
       "        -4.86380467e-03,  3.57662559e-01,  1.42409548e-01,  1.07767589e-01,\n",
       "         3.19250107e-01,  7.46258140e-01,  1.97833687e-01,  4.96163428e-01,\n",
       "        -2.99264133e-01,  7.13671148e-02,  3.80270422e-01,  2.91976392e-01,\n",
       "        -9.16748047e-02,  3.06402445e-01, -2.74511278e-02,  1.47132903e-01,\n",
       "        -4.36582804e-01, -2.03242730e-02,  6.39089775e+00, -2.12903887e-01,\n",
       "         1.58865660e-01,  1.36824563e-01, -3.64644639e-02, -1.27444908e-01,\n",
       "        -1.97010070e-01, -4.56434101e-01, -1.09690376e-01,  3.00060749e-01,\n",
       "        -5.12138307e-01, -3.25558156e-01, -4.48505096e-02, -6.06514513e-01,\n",
       "         1.98968574e-01,  9.67850909e-02, -1.73898444e-01,  1.51908025e-01,\n",
       "         3.86156961e-02, -2.14963451e-01,  7.73140267e-02,  1.48741588e-01,\n",
       "         8.47445548e-01,  9.56509948e-01, -1.85161576e-01, -2.56354302e-01,\n",
       "         2.33057484e-01, -6.15577772e-02,  9.87698883e-02,  1.23352818e-01,\n",
       "         1.99742258e-01, -5.45245588e-01, -6.10977300e-02,  1.17555775e-01,\n",
       "        -7.97724947e-02, -4.89029795e-01,  5.32559156e-01,  3.27499360e-01,\n",
       "         3.26782987e-02, -7.06863329e-02, -2.43443623e-01, -3.62279564e-01,\n",
       "        -1.28124669e-01,  6.20611012e-02, -2.46090338e-01, -7.29724690e-02,\n",
       "        -1.17798708e-01,  4.25636292e-01,  1.33455262e-01, -5.67164924e-03,\n",
       "         2.84783989e-01,  2.83298850e-01, -1.27889469e-01, -4.81734097e-01,\n",
       "         2.58203328e-01, -6.40906990e-01,  8.01770389e-02, -3.59539628e-01,\n",
       "        -2.61765242e-01,  8.45398754e-02,  1.84588611e-01, -1.71465769e-01,\n",
       "        -1.53534979e-01, -4.52104837e-01, -2.78893024e-01,  1.44667372e-01,\n",
       "        -8.49785563e-03, -5.10423221e-02,  3.47771794e-01,  3.85003269e-01],\n",
       "       dtype=float32)}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno_vit16[file_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0118404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_anno_file = {}\n",
    "for anno_few in all_list:\n",
    "#     print(anno_few)\n",
    "    file_n = anno_few['filename']\n",
    "    new_anno_file[file_n] = {}\n",
    "    pair_with_original = anno_few['pair_with_original']\n",
    "    \n",
    "#     print(anno_vit16[file_n].keys(),file_n)\n",
    "    for k in anno_vit16[file_n].keys():\n",
    "#         print(anno_vit16[file_n][k])\n",
    "        if k!= 'global_feature':\n",
    "            new_anno_file[file_n][k] = anno_vit16[file_n][k][pair_with_original]\n",
    "        else:\n",
    "            new_anno_file[file_n][k] = anno_vit16[file_n][k]\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dda8dbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('few_shot_pickle/few_shot_{}_{}_vit16.p'.format(sample_lens,sum(already_sampled)),'wb') as f:\n",
    "    pickle.dump(new_anno_file,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6ecfc868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boxes_h': [[206.0, 26.0, 404.0, 402.0], [337.0, 1.0, 533.0, 395.0]], 'boxes_o': [[102.0, 310.0, 296.0, 478.0], [102.0, 310.0, 296.0, 478.0]], 'hoi': [266, 266], 'object': [55, 55], 'verb': [15, 15]}\n"
     ]
    }
   ],
   "source": [
    "dicts = {}\n",
    "for idx_1 in idx:\n",
    "    new_dicts = {}\n",
    "    filename = _filenames[idx_1]\n",
    "   \n",
    "    new_dicts['boxes_h'] = _anno[idx_1]['boxes_h']\n",
    "    new_dicts['boxes_o'] = _anno[idx_1]['boxes_o']\n",
    "    print(_anno[idx_1])\n",
    "    break\n",
    "    dicts[filename] = new_dicts\n",
    "#     save_all_bbox.append(dicts)\n",
    "    \n",
    "#     print(_anno[idx_1])\n",
    "#     print(_image_sizes[idx_1])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2eef993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"save_bboxes_ye.p\",\"wb\") as f:\n",
    "    pickle.dump(dicts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d18e4348",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_boxes = torch.as_tensor(_anno[0]['boxes_h'])\n",
    "object_boxes = torch.as_tensor(_anno[0]['boxes_o'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3c79a9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 97.0667,  15.4000, 199.2667, 140.0000],\n",
       "        [ 99.4000,   9.3333, 204.4000, 166.6000],\n",
       "        [ 96.1333,  15.4000, 199.2667, 142.8000],\n",
       "        [ 97.5333,  12.1333, 207.2000, 147.9333]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_boxes * 224/480\n",
    "# object_boxes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a188f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3c49bb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import torch\n",
    "from torchvision.ops.boxes import batched_nms, box_iou\n",
    "import numpy as np\n",
    "from hico_list import hico_verb_object_list,hico_verbs,hico_verbs_sentence,hico_verbs_sentence_2\n",
    "import pdb\n",
    "HOI_IDX_TO_OBJ_IDX = [\n",
    "                4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 14,\n",
    "                14, 14, 14, 14, 14, 14, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 39,\n",
    "                39, 39, 39, 39, 39, 39, 39, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 2, 2, 2, 2,\n",
    "                2, 2, 2, 2, 2, 2, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 56, 56, 56, 56,\n",
    "                56, 56, 57, 57, 57, 57, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 60, 60,\n",
    "                60, 60, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
    "                16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 3,\n",
    "                3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 58,\n",
    "                58, 58, 58, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 6, 6, 6, 6, 6,\n",
    "                6, 6, 6, 62, 62, 62, 62, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 24, 24,\n",
    "                24, 24, 24, 24, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 34, 34, 34, 34, 34,\n",
    "                34, 34, 34, 35, 35, 35, 21, 21, 21, 21, 59, 59, 59, 59, 13, 13, 13, 13, 73,\n",
    "                73, 73, 73, 73, 45, 45, 45, 45, 45, 50, 50, 50, 50, 50, 50, 50, 55, 55, 55,\n",
    "                55, 55, 55, 55, 55, 55, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 67, 67, 67,\n",
    "                67, 67, 67, 67, 74, 74, 74, 74, 74, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41,\n",
    "                54, 54, 54, 54, 54, 54, 54, 54, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
    "                20, 10, 10, 10, 10, 10, 42, 42, 42, 42, 42, 42, 29, 29, 29, 29, 29, 29, 23,\n",
    "                23, 23, 23, 23, 23, 78, 78, 78, 78, 26, 26, 26, 26, 52, 52, 52, 52, 52, 52,\n",
    "                52, 66, 66, 66, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 43, 43, 43, 43, 43,\n",
    "                43, 43, 63, 63, 63, 63, 63, 63, 68, 68, 68, 68, 64, 64, 64, 64, 49, 49, 49,\n",
    "                49, 49, 49, 49, 49, 49, 49, 69, 69, 69, 69, 69, 69, 69, 12, 12, 12, 12, 53,\n",
    "                53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 72, 72, 72, 72, 72, 65, 65, 65, 65,\n",
    "                48, 48, 48, 48, 48, 48, 48, 76, 76, 76, 76, 71, 71, 71, 71, 36, 36, 36, 36,\n",
    "                36, 36, 36, 36, 36, 36, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31,\n",
    "                31, 31, 31, 31, 31, 31, 31, 44, 44, 44, 44, 44, 32, 32, 32, 32, 32, 32, 32,\n",
    "                32, 32, 32, 32, 32, 32, 32, 11, 11, 11, 11, 28, 28, 28, 28, 28, 28, 28, 28,\n",
    "                28, 28, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 77, 77, 77, 77, 77,\n",
    "                38, 38, 38, 38, 38, 27, 27, 27, 27, 27, 27, 27, 27, 70, 70, 70, 70, 61, 61,\n",
    "                61, 61, 61, 61, 61, 61, 79, 79, 79, 79, 9, 9, 9, 9, 9, 7, 7, 7, 7, 7, 7, 7,\n",
    "                7, 7, 25, 25, 25, 25, 25, 25, 25, 25, 75, 75, 75, 75, 40, 40, 40, 40, 40,\n",
    "                40, 40, 22, 22, 22, 22, 22\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3a44b084",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = 'union_embeddings_cachemodel_crop_padding_zeros_vitb16.p'\n",
    "class_nums = 600\n",
    "annotation = pickle.load(open(file1,'rb'))\n",
    "# if category == 'verb':\n",
    "categories = class_nums\n",
    "union_embeddings = [[] for i in range(categories)]\n",
    "obj_embeddings = [[] for i in range(categories)]\n",
    "hum_embeddings = [[] for i in range(categories)]\n",
    "filenames = list(annotation.keys())\n",
    "verbs_iou = [[] for i in range(class_nums)] # contain 600hois or 117 verbs\n",
    "# hois_iou = [[] for i in range(len(hois))]\n",
    "# filenames = [[] for i in range(class_nums)] # contain 600hois or 117 verbs\n",
    "each_filenames = [[] for i in range(categories)]\n",
    "sample_indexes = [[] for i in range(categories)]\n",
    "\n",
    "# play for unseen or inter swap\n",
    "verbs_human_feat = [[] for i in range(117)]\n",
    "\n",
    "gt_pair_hum_obj_area = [[] for i in range(categories)]\n",
    "gt_pair_hum_obj_area_ratio = [[] for i in range(categories)]\n",
    "\n",
    "obj_belong_to_object_pool = [[] for i in range(categories)]\n",
    "object_feat = [[] for i in range(80)]\n",
    "object_ref_area = [[] for i in range(80)]\n",
    "human_ref_area =  [[] for i in range(categories)]\n",
    "\n",
    "\n",
    "object_ref_area_ratio = [[] for i in range(80)]\n",
    "\n",
    "\n",
    "\n",
    "others_hum = [[] for i in range(categories)]\n",
    "count = 0\n",
    "for file_n in filenames:\n",
    "    anno = annotation[file_n]\n",
    "    if categories == 117: verbs = anno['verbs']\n",
    "    else: verbs = anno['hois']\n",
    "\n",
    "    union_features = anno['union_features']\n",
    "    object_features = anno['object_features']\n",
    "    # pdb.set_trace()\n",
    "    huamn_features = anno['huamn_features']\n",
    "    gt_hum_boxes = torch.as_tensor(anno['boxes_h'])\n",
    "    gt_obj_boxes = torch.as_tensor(anno['boxes_o'])                            \n",
    "                                   \n",
    "    ious = torch.diag(box_iou(gt_hum_boxes, gt_obj_boxes))\n",
    "    # pdb.set_trace()\n",
    "    x, y = torch.nonzero(torch.min(\n",
    "        box_iou(gt_hum_boxes, gt_hum_boxes),\n",
    "        box_iou(gt_obj_boxes, gt_obj_boxes),\n",
    "        ) >= 0.5).unbind(1)\n",
    "    # \n",
    "\n",
    "#     pdb.set_trace()\n",
    "    orig_verbs = anno['verbs']\n",
    "    objects_label = torch.as_tensor(anno['objects'])\n",
    "\n",
    "    \n",
    "    area1 = (gt_hum_boxes[:,2]-gt_hum_boxes[:,0]) * (gt_hum_boxes[:,3]-gt_hum_boxes[:,1])\n",
    "    area2 = (gt_obj_boxes[:,2]-gt_obj_boxes[:,0]) * (gt_obj_boxes[:,3]-gt_obj_boxes[:,1])\n",
    "    area_ratio =   area2/ area1                  \n",
    "    if len(verbs) == 0:\n",
    "        print(file_n)\n",
    "    # if torch.sum(objects_label[0]==objects_label) != objects_label.shape[0]:\n",
    "    #     print(objects_label)\n",
    "    #     pdb.set_trace()\n",
    "    count+=1\n",
    "    for i, v in enumerate(verbs):\n",
    "\n",
    "        union_embeddings[v].append(union_features[i] / np.linalg.norm(union_features[i]))\n",
    "        obj_embeddings[v].append(object_features[i] / np.linalg.norm(object_features[i]))\n",
    "        hum_embeddings[v].append(huamn_features[i] / np.linalg.norm(huamn_features[i]))\n",
    "        each_filenames[v].append(file_n)\n",
    "        sample_indexes[v].append(i)\n",
    "        gt_pair_hum_obj_area[v].append((area1[i],area2[i]))\n",
    "#         object_ref_area_ratio[v].append(area_ratio[i])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ind_x = torch.where(x==i)[0]\n",
    "        ind_y = torch.where(y[ind_x]!=i)[0]\n",
    "        object_pair = torch.where(objects_label[i]==objects_label[ind_y])[0]\n",
    "        ind_y_new = ind_y[object_pair]\n",
    "        if len(ind_y_new) == 1:\n",
    "            # ind_y_new = ind_y_new[None,]\n",
    "            others_hum[v].append(torch.as_tensor(huamn_features[ind_y_new][None,]))\n",
    "            # pdb.set_trace()\n",
    "        else:\n",
    "            others_hum[v].append(torch.as_tensor(huamn_features[ind_y_new]))\n",
    "            \n",
    "        object_ref_area[objects_label[i]].append(area2[i])\n",
    "        human_ref_area[v].append(area1[ind_y_new])\n",
    "        \n",
    "        verbs_iou[v].append(ious[i])\n",
    "#         if v in self.unseen_nonrare_first and self.unseen_setting:\n",
    "#             # pdb.set_trace()\n",
    "#             continue\n",
    "        verbs_human_feat[orig_verbs[i]].append(huamn_features[i] / np.linalg.norm(huamn_features[i]))\n",
    "        obj_belong_to_object_pool[v].append(len(object_feat[objects_label[i]]))\n",
    "        object_feat[objects_label[i]].append(object_features[i] / np.linalg.norm(object_features[i]))\n",
    "        \n",
    "        # add iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6535c543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(obj_belong_to_object_pool[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "854ddbb7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1811])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(object_ref_area[0]).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9e6651",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat(object_feat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0aaf0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_ref_embs = torch.stack([torch.as_tensor(obj) for obj in object_feat[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b45d988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1811, 512])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_ref_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "082b3e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_hum = torch.cat(others_hum[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92180f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 512])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_hum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "959c2c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hum_area = torch.stack([gt_pair[0] for gt_pair in gt_pair_hum_obj_area[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5cf10c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_area = torch.cat(human_ref_area[0])\n",
    "ref_area.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "65b88239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1811])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(object_ref_area[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f3c855f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([29408., 29408., 29408., 29408., 29408., 33312., 28112., 28112., 28112.,\n",
       "        28112., 28112., 28112., 28112., 28112., 28112., 28112., 28112., 28112.,\n",
       "        28112., 28112., 28112., 28112., 28112., 28112., 28112., 28112., 28112.,\n",
       "        28112., 28112., 28112., 28112., 28112., 22288., 22288., 22288., 22288.,\n",
       "        22288., 22288., 22288., 25520., 25520., 25520., 42720., 42720., 42720.,\n",
       "        42720., 37664., 41888., 13288., 13288., 13288., 13288., 13288., 20928.,\n",
       "        25376., 25376., 25376., 25376., 26688., 26688., 26688., 26688., 26688.,\n",
       "        26688., 26688., 26688., 26688., 26688., 26688., 29984., 29984., 48672.,\n",
       "        48672., 48672., 28512., 28512., 28512., 32016., 28944., 22544., 11816.,\n",
       "        11816., 11816., 11816., 11816., 11816., 11816., 11816., 11816., 11816.,\n",
       "        11816., 11816., 11816., 11816., 41440., 14688., 14688., 26432., 15872.,\n",
       "        30032., 30032., 30032., 22592., 22592., 22592., 22592., 22592., 22592.,\n",
       "        22592., 22592., 22592., 20320., 27984., 27984., 17616., 17616., 17616.,\n",
       "        17616., 17616., 17616., 17616., 30080., 17632., 30256., 36352., 20800.,\n",
       "        20112., 20112., 20112., 24224., 24224., 24224., 24224., 33824., 35680.,\n",
       "        35776., 26016., 26016., 27664., 17904., 17904., 17904., 41952., 38048.,\n",
       "        11304., 11304., 11304., 11304., 11304., 11304., 11304., 11304., 11304.,\n",
       "        11304., 18192., 25408., 31168., 31168., 31168., 31168., 11656., 11656.,\n",
       "        11656., 11656., 11656., 11656., 11656., 39296., 39296., 39296., 21376.,\n",
       "        27824., 27824., 27824., 27824., 21840.], dtype=torch.float16)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea99cbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_models = []\n",
    "one_hots = []\n",
    "each_lens = []\n",
    "indexes = np.arange(len(union_embeddings))\n",
    "save_all_list = []\n",
    "for i, hum_emb, obj_emb, embeddings in zip(indexes, hum_embeddings, obj_embeddings, union_embeddings):\n",
    "    range_lens = np.arange(len(embeddings))\n",
    "    hum_emb = torch.as_tensor(hum_emb)\n",
    "    obj_emb = torch.as_tensor(obj_emb)\n",
    "    hum_area = torch.stack([gt_pair[0] for gt_pair in gt_pair_hum_obj_area[i]])\n",
    "    obj_area = torch.stack([gt_pair[1] for gt_pair in gt_pair_hum_obj_area[i]])\n",
    "    \n",
    "    lens = len(hum_emb)\n",
    "\n",
    "    indexes = torch.arange(0,lens)[:,None]\n",
    "    ref_hum = torch.cat(others_hum[i])\n",
    "    ref_hum = ref_hum/ ref_hum.norm(dim=-1, keepdim=True)\n",
    "    ref_area = torch.cat(human_ref_area[i])\n",
    "    \n",
    "    new_hum = torch.cat([hum_emb,ref_hum])\n",
    "    sim_ = new_hum @ new_hum.t()\n",
    "#     pdb.set_trace()\n",
    "    new_hum_area = torch.cat([hum_area, ref_area],dim=0)\n",
    "    \n",
    "    # object \n",
    "    sim_obj = obj_emb @ object_ref_embs.t()\n",
    "    \n",
    "    object_cls = HOI_IDX_TO_OBJ_IDX[i]\n",
    "    object_ref_embs = torch.stack([torch.as_tensor(obj) for obj in object_feat[object_cls]])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ref_obj_area = torch.stack(object_ref_area[object_cls])\n",
    "    object_indexes_pool = obj_belong_to_object_pool[i]\n",
    "    \n",
    "    save_one_hum = []\n",
    "    save_one_obj = []\n",
    "    for k, sim in enumerate(sim_):\n",
    "#         pdb.set_trace()\n",
    "        # rule1: human simialrity\n",
    "        sort_indexes = sim.sort(descending=True)[1]\n",
    "        valid_indexes = sort_indexes[torch.where(sort_indexes != k)[0]]\n",
    "        sample_lens = min(len(valid_indexes),5)\n",
    "        sample_hum_feats = new_hum[valid_indexes][:sample_lens]\n",
    "        \n",
    "        # object\n",
    "        obj_area_ = obj_area[k]\n",
    "        obj_ratio_ = torch.abs(ref_obj_area/obj_area_  -1)\n",
    "        sort_obj_indexes = obj_ratio_.sort()[1]\n",
    "        current_obj = object_indexes_pool[k]\n",
    "        valid_obj_indexes = sort_obj_indexes[torch.where(sort_obj_indexes != current_obj)[0]]\n",
    "        sample_obj_lens = min(len(valid_obj_indexes),5)\n",
    "        sample_obj_feats = object_ref_embs[valid_obj_indexes][:sample_obj_lens]\n",
    "        \n",
    "        final_hum_feats = sample_hum_feats.unsqueeze(1).repeat(1,sample_obj_lens,1).view(-1,512)\n",
    "        final_obj_feats = sample_obj_feats.unsqueeze(0).repeat(sample_lens,1,1).view(-1,512)\n",
    "        save_one_hum.append(final_hum_feats)\n",
    "        save_one_obj.append(final_obj_feats)\n",
    "    \n",
    "    pdb.set_trace()\n",
    "    final_hum = torch.cat([hum_emb,torch.cat(save_one_hum)])\n",
    "    final_obj = torch.cat([obj_emb,torch.cat(save_one_obj)])\n",
    "    gt_sample_lens = lens\n",
    "    new_dict = {}\n",
    "    new_dict['final_hum'] = final_hum\n",
    "    new_dict['final_obj'] = final_obj\n",
    "    new_dict['gt_sample_lens'] = gt_sample_lens\n",
    "    save_all_list.append(new_dict)\n",
    "#     topk_lens = min(6, len(sim_))\n",
    "#     topk_sample = sim_.topk(topk_lens,dim=-1)[1][:lens] \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     # ref_x, ref_y = torch.nonzero(topk_sample != indexes).unbind(1)\n",
    "#     # final_indexes = topk_sample[ref_x,ref_y].reshape(-1,topk_lens-1)\n",
    "#     # ref_hum_embs = new_hum[final_indexes].unsqueeze(1).repeat(1,topk_lens-1,1,1)\n",
    "#     ref_hum_embs = new_hum[topk_sample[:,1:]].unsqueeze(1).repeat(1,topk_lens-1,1,1)\n",
    "#     print(\"original lens: {}, augmented lens:{}, label:{}\".format(lens,ref_hum_embs.shape[1]*ref_hum_embs.shape[0], hico_verb_object_list[i]))\n",
    "#     pdb.set_trace()\n",
    "\n",
    "#     object_cls = self.HOI_IDX_TO_OBJ_IDX[i]\n",
    "#     object_ref_embs = torch.stack([torch.as_tensor(obj) for obj in object_feat[object_cls]])\n",
    "    \n",
    "#     sim_obj = obj_emb @ object_ref_embs.t()\n",
    "#     topk_sample_obj = sim_obj.topk(topk_lens-1,dim=-1)[1][:lens]\n",
    "#     ref_obj_embs = object_ref_embs[topk_sample_obj].unsqueeze(2).repeat(1,1,topk_lens-1,1)\n",
    "\n",
    "#     hum_emb = torch.cat([hum_emb,ref_hum_embs.view(-1,512)],dim=0)\n",
    "#     obj_emb = torch.cat([obj_emb,ref_obj_embs.view(-1,512)],dim=0)\n",
    "\n",
    "#     new_lens = len(hum_emb)\n",
    "\n",
    "#     new_K_shot = min(new_lens, K_shot)\n",
    "#     sample_index = np.random.choice(new_lens,new_K_shot,replace=False)\n",
    "\n",
    "#     sample_hum_embeddings =  hum_emb[sample_index]\n",
    "#     sample_obj_embeddings =  obj_emb[sample_index]\n",
    "#     save_embs = torch.cat([hum_emb,obj_emb],dim=-1)\n",
    "#     # pdb.set_trace()\n",
    "#     if i in self.unseen_rare_first and len(save_embs)>=2:\n",
    "#         # pdb.set_trace()\n",
    "#         save_dir = 'save_visual/{}_{}.png'.format('_'.join(self.hico_verb_object_list[i]),lens)\n",
    "#         self.draw_pic(save_embs, lens, save_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "281f3ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([    7,    31,    77,    92,   102,   102,   102,   121,   146,   177,\n",
       "          185,   186,   200,   213,   215,   262,   272,   275,   287,   332,\n",
       "          408,   417,   419,   429,   441,   448,   470,   504,   509,   517,\n",
       "          529,   529,   562,   572,   606,   706,   717,   735,   800,   868,\n",
       "          974,   988,  1057,  1086,  1131,  1178,  1207,  1224,  1236,  1238,\n",
       "         1277,  1337,  1358,  1386,  1396,  1447,  1451,  1479,  1642,  1665,\n",
       "         1675,  1689,  1811,  1838,  1863,  2228,  2345,  2352,  2415,  2424,\n",
       "         2622,  2949,  3587,  4396,  4889,  5594,  5787,  7249,  9462, 10369]),\n",
       "indices=tensor([70, 68, 11, 22, 72, 71, 78, 50, 64, 45, 21, 49, 61, 69, 75, 58, 65, 26,\n",
       "        10, 12, 35, 66, 42, 47, 51, 48, 15, 44, 76, 59, 62, 77, 38, 79, 74, 53,\n",
       "        23, 52,  9, 54, 14, 40, 41,  7, 19, 37, 29, 57, 27, 55, 67, 63, 46, 16,\n",
       "         6, 28, 43, 39, 20,  4, 18, 32,  0, 56, 34, 24, 33, 13,  2, 73, 31, 30,\n",
       "         5, 25, 17, 36, 60,  3,  1,  8]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value, index = torch.as_tensor([len(i) for i in object_feat]).sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "46f4388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hico_obj_text_label = [(0, 'a photo of a person'), (1, 'a photo of a bicycle'), (2, 'a photo of a car'),\n",
    "                       (3, 'a photo of a motorcycle'), (4, 'a photo of an airplane'), (5, 'a photo of a bus'),\n",
    "                       (6, 'a photo of a train'), (7, 'a photo of a truck'), (8, 'a photo of a boat'),\n",
    "                       (9, 'a photo of a traffic light'), (10, 'a photo of a fire hydrant'),\n",
    "                       (11, 'a photo of a stop sign'), (12, 'a photo of a parking meter'), (13, 'a photo of a bench'),\n",
    "                       (14, 'a photo of a bird'), (15, 'a photo of a cat'), (16, 'a photo of a dog'),\n",
    "                       (17, 'a photo of a horse'), (18, 'a photo of a sheep'), (19, 'a photo of a cow'),\n",
    "                       (20, 'a photo of an elephant'), (21, 'a photo of a bear'), (22, 'a photo of a zebra'),\n",
    "                       (23, 'a photo of a giraffe'), (24, 'a photo of a backpack'), (25, 'a photo of a umbrella'),\n",
    "                       (26, 'a photo of a handbag'), (27, 'a photo of a tie'), (28, 'a photo of a suitcase'),\n",
    "                       (29, 'a photo of a frisbee'), (30, 'a photo of a skis'), (31, 'a photo of a snowboard'),\n",
    "                       (32, 'a photo of a sports ball'), (33, 'a photo of a kite'), (34, 'a photo of a baseball bat'),\n",
    "                       (35, 'a photo of a baseball glove'), (36, 'a photo of a skateboard'),\n",
    "                       (37, 'a photo of a surfboard'), (38, 'a photo of a tennis racket'), (39, 'a photo of a bottle'),\n",
    "                       (40, 'a photo of a wine glass'), (41, 'a photo of a cup'), (42, 'a photo of a fork'),\n",
    "                       (43, 'a photo of a knife'), (44, 'a photo of a spoon'), (45, 'a photo of a bowl'),\n",
    "                       (46, 'a photo of a banana'), (47, 'a photo of an apple'), (48, 'a photo of a sandwich'),\n",
    "                       (49, 'a photo of an orange'), (50, 'a photo of a broccoli'), (51, 'a photo of a carrot'),\n",
    "                       (52, 'a photo of a hot dog'), (53, 'a photo of a pizza'), (54, 'a photo of a donut'),\n",
    "                       (55, 'a photo of a cake'), (56, 'a photo of a chair'), (57, 'a photo of a couch'),\n",
    "                       (58, 'a photo of a potted plant'), (59, 'a photo of a bed'), (60, 'a photo of a dining table'),\n",
    "                       (61, 'a photo of a toilet'), (62, 'a photo of a tv'), (63, 'a photo of a laptop'),\n",
    "                       (64, 'a photo of a mouse'), (65, 'a photo of a remote'), (66, 'a photo of a keyboard'),\n",
    "                       (67, 'a photo of a cell phone'), (68, 'a photo of a microwave'), (69, 'a photo of an oven'),\n",
    "                       (70, 'a photo of a toaster'), (71, 'a photo of a sink'), (72, 'a photo of a refrigerator'),\n",
    "                       (73, 'a photo of a book'), (74, 'a photo of a clock'), (75, 'a photo of a vase'),\n",
    "                       (76, 'a photo of a scissors'), (77, 'a photo of a teddy bear'), (78, 'a photo of a hair drier'),\n",
    "                       (79, 'a photo of a toothbrush'), (80, 'a photo of nothing')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "10e7ba4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('direct', 'car')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hico_verb_object_list[66]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f52039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qpic",
   "language": "python",
   "name": "qpic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
