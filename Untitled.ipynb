{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0e61245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "anno_file = 'hicodet/instances_train2015.json'\n",
    "with open(anno_file, 'r') as f:\n",
    "    anno = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ae846420",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = list(range(len(anno['filenames'])))\n",
    "for empty_idx in anno['empty']:\n",
    "    idx.remove(empty_idx)\n",
    "random.shuffle(idx)\n",
    "# num_anno = [0 for _ in range(self.num_interation_cls)]\n",
    "# for anno in f['annotation']:\n",
    "#     for hoi in anno['hoi']:\n",
    "#         num_anno[hoi] += 1\n",
    "\n",
    "_idx = idx\n",
    "# _num_anno = num_anno\n",
    "\n",
    "_anno = anno['annotation']\n",
    "_filenames = anno['filenames']\n",
    "_image_sizes = anno['size']\n",
    "_class_corr = anno['correspondence']\n",
    "_empty_idx = anno['empty']\n",
    "_objects = anno['objects']\n",
    "_verbs = anno['verbs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "73184582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16785"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff005fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_lens = 2\n",
    "dicts_ = {}\n",
    "already_sampled = [0 for i in range(600)]\n",
    "for i in idx:\n",
    "    anno_one = _anno[i]\n",
    "    filename = _filenames[i]\n",
    "    size = _image_sizes[i]\n",
    "    \n",
    "    hois = anno_one['hoi']\n",
    "    objects = \n",
    "    objects = \n",
    "    boxes_h = anno_one['boxes_h']\n",
    "    boxes_o = anno_one['boxes_o']\n",
    "    \n",
    "    \n",
    "    all_boxes_h = []\n",
    "    all_boxes_o = []\n",
    "    all_hois = []\n",
    "    all_objects = []\n",
    "    all_verbs = []\n",
    "    for hoi in hois:\n",
    "        if already_sampled[hoi] != sample_lens:\n",
    "            already_sampled[hoi] +=1\n",
    "            \n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b4f1e6d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38118"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(_anno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fa39d0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37633"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f54474a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38118"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0118404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_all_bbox = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6ecfc868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boxes_h': [[206.0, 26.0, 404.0, 402.0], [337.0, 1.0, 533.0, 395.0]], 'boxes_o': [[102.0, 310.0, 296.0, 478.0], [102.0, 310.0, 296.0, 478.0]], 'hoi': [266, 266], 'object': [55, 55], 'verb': [15, 15]}\n"
     ]
    }
   ],
   "source": [
    "dicts = {}\n",
    "for idx_1 in idx:\n",
    "    new_dicts = {}\n",
    "    filename = _filenames[idx_1]\n",
    "   \n",
    "    new_dicts['boxes_h'] = _anno[idx_1]['boxes_h']\n",
    "    new_dicts['boxes_o'] = _anno[idx_1]['boxes_o']\n",
    "    print(_anno[idx_1])\n",
    "    break\n",
    "    dicts[filename] = new_dicts\n",
    "#     save_all_bbox.append(dicts)\n",
    "    \n",
    "#     print(_anno[idx_1])\n",
    "#     print(_image_sizes[idx_1])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2eef993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"save_bboxes_ye.p\",\"wb\") as f:\n",
    "    pickle.dump(dicts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d18e4348",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_boxes = torch.as_tensor(_anno[0]['boxes_h'])\n",
    "object_boxes = torch.as_tensor(_anno[0]['boxes_o'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3c79a9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 97.0667,  15.4000, 199.2667, 140.0000],\n",
       "        [ 99.4000,   9.3333, 204.4000, 166.6000],\n",
       "        [ 96.1333,  15.4000, 199.2667, 142.8000],\n",
       "        [ 97.5333,  12.1333, 207.2000, 147.9333]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_boxes * 224/480\n",
    "# object_boxes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a188f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3c49bb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import torch\n",
    "from torchvision.ops.boxes import batched_nms, box_iou\n",
    "import numpy as np\n",
    "from hico_list import hico_verb_object_list,hico_verbs,hico_verbs_sentence,hico_verbs_sentence_2\n",
    "import pdb\n",
    "HOI_IDX_TO_OBJ_IDX = [\n",
    "                4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 14,\n",
    "                14, 14, 14, 14, 14, 14, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 39,\n",
    "                39, 39, 39, 39, 39, 39, 39, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 2, 2, 2, 2,\n",
    "                2, 2, 2, 2, 2, 2, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 56, 56, 56, 56,\n",
    "                56, 56, 57, 57, 57, 57, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 60, 60,\n",
    "                60, 60, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
    "                16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 3,\n",
    "                3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 58,\n",
    "                58, 58, 58, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 6, 6, 6, 6, 6,\n",
    "                6, 6, 6, 62, 62, 62, 62, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 24, 24,\n",
    "                24, 24, 24, 24, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 34, 34, 34, 34, 34,\n",
    "                34, 34, 34, 35, 35, 35, 21, 21, 21, 21, 59, 59, 59, 59, 13, 13, 13, 13, 73,\n",
    "                73, 73, 73, 73, 45, 45, 45, 45, 45, 50, 50, 50, 50, 50, 50, 50, 55, 55, 55,\n",
    "                55, 55, 55, 55, 55, 55, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 67, 67, 67,\n",
    "                67, 67, 67, 67, 74, 74, 74, 74, 74, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41,\n",
    "                54, 54, 54, 54, 54, 54, 54, 54, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
    "                20, 10, 10, 10, 10, 10, 42, 42, 42, 42, 42, 42, 29, 29, 29, 29, 29, 29, 23,\n",
    "                23, 23, 23, 23, 23, 78, 78, 78, 78, 26, 26, 26, 26, 52, 52, 52, 52, 52, 52,\n",
    "                52, 66, 66, 66, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 43, 43, 43, 43, 43,\n",
    "                43, 43, 63, 63, 63, 63, 63, 63, 68, 68, 68, 68, 64, 64, 64, 64, 49, 49, 49,\n",
    "                49, 49, 49, 49, 49, 49, 49, 69, 69, 69, 69, 69, 69, 69, 12, 12, 12, 12, 53,\n",
    "                53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 72, 72, 72, 72, 72, 65, 65, 65, 65,\n",
    "                48, 48, 48, 48, 48, 48, 48, 76, 76, 76, 76, 71, 71, 71, 71, 36, 36, 36, 36,\n",
    "                36, 36, 36, 36, 36, 36, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31,\n",
    "                31, 31, 31, 31, 31, 31, 31, 44, 44, 44, 44, 44, 32, 32, 32, 32, 32, 32, 32,\n",
    "                32, 32, 32, 32, 32, 32, 32, 11, 11, 11, 11, 28, 28, 28, 28, 28, 28, 28, 28,\n",
    "                28, 28, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 77, 77, 77, 77, 77,\n",
    "                38, 38, 38, 38, 38, 27, 27, 27, 27, 27, 27, 27, 27, 70, 70, 70, 70, 61, 61,\n",
    "                61, 61, 61, 61, 61, 61, 79, 79, 79, 79, 9, 9, 9, 9, 9, 7, 7, 7, 7, 7, 7, 7,\n",
    "                7, 7, 25, 25, 25, 25, 25, 25, 25, 25, 75, 75, 75, 75, 40, 40, 40, 40, 40,\n",
    "                40, 40, 22, 22, 22, 22, 22\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3a44b084",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = 'union_embeddings_cachemodel_crop_padding_zeros_vitb16.p'\n",
    "class_nums = 600\n",
    "annotation = pickle.load(open(file1,'rb'))\n",
    "# if category == 'verb':\n",
    "categories = class_nums\n",
    "union_embeddings = [[] for i in range(categories)]\n",
    "obj_embeddings = [[] for i in range(categories)]\n",
    "hum_embeddings = [[] for i in range(categories)]\n",
    "filenames = list(annotation.keys())\n",
    "verbs_iou = [[] for i in range(class_nums)] # contain 600hois or 117 verbs\n",
    "# hois_iou = [[] for i in range(len(hois))]\n",
    "# filenames = [[] for i in range(class_nums)] # contain 600hois or 117 verbs\n",
    "each_filenames = [[] for i in range(categories)]\n",
    "sample_indexes = [[] for i in range(categories)]\n",
    "\n",
    "# play for unseen or inter swap\n",
    "verbs_human_feat = [[] for i in range(117)]\n",
    "\n",
    "gt_pair_hum_obj_area = [[] for i in range(categories)]\n",
    "gt_pair_hum_obj_area_ratio = [[] for i in range(categories)]\n",
    "\n",
    "obj_belong_to_object_pool = [[] for i in range(categories)]\n",
    "object_feat = [[] for i in range(80)]\n",
    "object_ref_area = [[] for i in range(80)]\n",
    "human_ref_area =  [[] for i in range(categories)]\n",
    "\n",
    "\n",
    "object_ref_area_ratio = [[] for i in range(80)]\n",
    "\n",
    "\n",
    "\n",
    "others_hum = [[] for i in range(categories)]\n",
    "count = 0\n",
    "for file_n in filenames:\n",
    "    anno = annotation[file_n]\n",
    "    if categories == 117: verbs = anno['verbs']\n",
    "    else: verbs = anno['hois']\n",
    "\n",
    "    union_features = anno['union_features']\n",
    "    object_features = anno['object_features']\n",
    "    # pdb.set_trace()\n",
    "    huamn_features = anno['huamn_features']\n",
    "    gt_hum_boxes = torch.as_tensor(anno['boxes_h'])\n",
    "    gt_obj_boxes = torch.as_tensor(anno['boxes_o'])                            \n",
    "                                   \n",
    "    ious = torch.diag(box_iou(gt_hum_boxes, gt_obj_boxes))\n",
    "    # pdb.set_trace()\n",
    "    x, y = torch.nonzero(torch.min(\n",
    "        box_iou(gt_hum_boxes, gt_hum_boxes),\n",
    "        box_iou(gt_obj_boxes, gt_obj_boxes),\n",
    "        ) >= 0.5).unbind(1)\n",
    "    # \n",
    "\n",
    "#     pdb.set_trace()\n",
    "    orig_verbs = anno['verbs']\n",
    "    objects_label = torch.as_tensor(anno['objects'])\n",
    "\n",
    "    \n",
    "    area1 = (gt_hum_boxes[:,2]-gt_hum_boxes[:,0]) * (gt_hum_boxes[:,3]-gt_hum_boxes[:,1])\n",
    "    area2 = (gt_obj_boxes[:,2]-gt_obj_boxes[:,0]) * (gt_obj_boxes[:,3]-gt_obj_boxes[:,1])\n",
    "    area_ratio =   area2/ area1                  \n",
    "    if len(verbs) == 0:\n",
    "        print(file_n)\n",
    "    # if torch.sum(objects_label[0]==objects_label) != objects_label.shape[0]:\n",
    "    #     print(objects_label)\n",
    "    #     pdb.set_trace()\n",
    "    count+=1\n",
    "    for i, v in enumerate(verbs):\n",
    "\n",
    "        union_embeddings[v].append(union_features[i] / np.linalg.norm(union_features[i]))\n",
    "        obj_embeddings[v].append(object_features[i] / np.linalg.norm(object_features[i]))\n",
    "        hum_embeddings[v].append(huamn_features[i] / np.linalg.norm(huamn_features[i]))\n",
    "        each_filenames[v].append(file_n)\n",
    "        sample_indexes[v].append(i)\n",
    "        gt_pair_hum_obj_area[v].append((area1[i],area2[i]))\n",
    "#         object_ref_area_ratio[v].append(area_ratio[i])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ind_x = torch.where(x==i)[0]\n",
    "        ind_y = torch.where(y[ind_x]!=i)[0]\n",
    "        object_pair = torch.where(objects_label[i]==objects_label[ind_y])[0]\n",
    "        ind_y_new = ind_y[object_pair]\n",
    "        if len(ind_y_new) == 1:\n",
    "            # ind_y_new = ind_y_new[None,]\n",
    "            others_hum[v].append(torch.as_tensor(huamn_features[ind_y_new][None,]))\n",
    "            # pdb.set_trace()\n",
    "        else:\n",
    "            others_hum[v].append(torch.as_tensor(huamn_features[ind_y_new]))\n",
    "            \n",
    "        object_ref_area[objects_label[i]].append(area2[i])\n",
    "        human_ref_area[v].append(area1[ind_y_new])\n",
    "        \n",
    "        verbs_iou[v].append(ious[i])\n",
    "#         if v in self.unseen_nonrare_first and self.unseen_setting:\n",
    "#             # pdb.set_trace()\n",
    "#             continue\n",
    "        verbs_human_feat[orig_verbs[i]].append(huamn_features[i] / np.linalg.norm(huamn_features[i]))\n",
    "        obj_belong_to_object_pool[v].append(len(object_feat[objects_label[i]]))\n",
    "        object_feat[objects_label[i]].append(object_features[i] / np.linalg.norm(object_features[i]))\n",
    "        \n",
    "        # add iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6535c543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(obj_belong_to_object_pool[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "854ddbb7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1811])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(object_ref_area[0]).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9e6651",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat(object_feat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0aaf0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_ref_embs = torch.stack([torch.as_tensor(obj) for obj in object_feat[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b45d988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1811, 512])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_ref_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "082b3e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_hum = torch.cat(others_hum[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92180f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 512])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_hum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "959c2c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hum_area = torch.stack([gt_pair[0] for gt_pair in gt_pair_hum_obj_area[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5cf10c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_area = torch.cat(human_ref_area[0])\n",
    "ref_area.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "65b88239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1811])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(object_ref_area[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f3c855f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([29408., 29408., 29408., 29408., 29408., 33312., 28112., 28112., 28112.,\n",
       "        28112., 28112., 28112., 28112., 28112., 28112., 28112., 28112., 28112.,\n",
       "        28112., 28112., 28112., 28112., 28112., 28112., 28112., 28112., 28112.,\n",
       "        28112., 28112., 28112., 28112., 28112., 22288., 22288., 22288., 22288.,\n",
       "        22288., 22288., 22288., 25520., 25520., 25520., 42720., 42720., 42720.,\n",
       "        42720., 37664., 41888., 13288., 13288., 13288., 13288., 13288., 20928.,\n",
       "        25376., 25376., 25376., 25376., 26688., 26688., 26688., 26688., 26688.,\n",
       "        26688., 26688., 26688., 26688., 26688., 26688., 29984., 29984., 48672.,\n",
       "        48672., 48672., 28512., 28512., 28512., 32016., 28944., 22544., 11816.,\n",
       "        11816., 11816., 11816., 11816., 11816., 11816., 11816., 11816., 11816.,\n",
       "        11816., 11816., 11816., 11816., 41440., 14688., 14688., 26432., 15872.,\n",
       "        30032., 30032., 30032., 22592., 22592., 22592., 22592., 22592., 22592.,\n",
       "        22592., 22592., 22592., 20320., 27984., 27984., 17616., 17616., 17616.,\n",
       "        17616., 17616., 17616., 17616., 30080., 17632., 30256., 36352., 20800.,\n",
       "        20112., 20112., 20112., 24224., 24224., 24224., 24224., 33824., 35680.,\n",
       "        35776., 26016., 26016., 27664., 17904., 17904., 17904., 41952., 38048.,\n",
       "        11304., 11304., 11304., 11304., 11304., 11304., 11304., 11304., 11304.,\n",
       "        11304., 18192., 25408., 31168., 31168., 31168., 31168., 11656., 11656.,\n",
       "        11656., 11656., 11656., 11656., 11656., 39296., 39296., 39296., 21376.,\n",
       "        27824., 27824., 27824., 27824., 21840.], dtype=torch.float16)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea99cbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_models = []\n",
    "one_hots = []\n",
    "each_lens = []\n",
    "indexes = np.arange(len(union_embeddings))\n",
    "save_all_list = []\n",
    "for i, hum_emb, obj_emb, embeddings in zip(indexes, hum_embeddings, obj_embeddings, union_embeddings):\n",
    "    range_lens = np.arange(len(embeddings))\n",
    "    hum_emb = torch.as_tensor(hum_emb)\n",
    "    obj_emb = torch.as_tensor(obj_emb)\n",
    "    hum_area = torch.stack([gt_pair[0] for gt_pair in gt_pair_hum_obj_area[i]])\n",
    "    obj_area = torch.stack([gt_pair[1] for gt_pair in gt_pair_hum_obj_area[i]])\n",
    "    \n",
    "    lens = len(hum_emb)\n",
    "\n",
    "    indexes = torch.arange(0,lens)[:,None]\n",
    "    ref_hum = torch.cat(others_hum[i])\n",
    "    ref_hum = ref_hum/ ref_hum.norm(dim=-1, keepdim=True)\n",
    "    ref_area = torch.cat(human_ref_area[i])\n",
    "    \n",
    "    new_hum = torch.cat([hum_emb,ref_hum])\n",
    "    sim_ = new_hum @ new_hum.t()\n",
    "#     pdb.set_trace()\n",
    "    new_hum_area = torch.cat([hum_area, ref_area],dim=0)\n",
    "    \n",
    "    # object \n",
    "    sim_obj = obj_emb @ object_ref_embs.t()\n",
    "    \n",
    "    object_cls = HOI_IDX_TO_OBJ_IDX[i]\n",
    "    object_ref_embs = torch.stack([torch.as_tensor(obj) for obj in object_feat[object_cls]])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ref_obj_area = torch.stack(object_ref_area[object_cls])\n",
    "    object_indexes_pool = obj_belong_to_object_pool[i]\n",
    "    \n",
    "    save_one_hum = []\n",
    "    save_one_obj = []\n",
    "    for k, sim in enumerate(sim_):\n",
    "#         pdb.set_trace()\n",
    "        # rule1: human simialrity\n",
    "        sort_indexes = sim.sort(descending=True)[1]\n",
    "        valid_indexes = sort_indexes[torch.where(sort_indexes != k)[0]]\n",
    "        sample_lens = min(len(valid_indexes),5)\n",
    "        sample_hum_feats = new_hum[valid_indexes][:sample_lens]\n",
    "        \n",
    "        # object\n",
    "        obj_area_ = obj_area[k]\n",
    "        obj_ratio_ = torch.abs(ref_obj_area/obj_area_  -1)\n",
    "        sort_obj_indexes = obj_ratio_.sort()[1]\n",
    "        current_obj = object_indexes_pool[k]\n",
    "        valid_obj_indexes = sort_obj_indexes[torch.where(sort_obj_indexes != current_obj)[0]]\n",
    "        sample_obj_lens = min(len(valid_obj_indexes),5)\n",
    "        sample_obj_feats = object_ref_embs[valid_obj_indexes][:sample_obj_lens]\n",
    "        \n",
    "        final_hum_feats = sample_hum_feats.unsqueeze(1).repeat(1,sample_obj_lens,1).view(-1,512)\n",
    "        final_obj_feats = sample_obj_feats.unsqueeze(0).repeat(sample_lens,1,1).view(-1,512)\n",
    "        save_one_hum.append(final_hum_feats)\n",
    "        save_one_obj.append(final_obj_feats)\n",
    "    \n",
    "    pdb.set_trace()\n",
    "    final_hum = torch.cat([hum_emb,torch.cat(save_one_hum)])\n",
    "    final_obj = torch.cat([obj_emb,torch.cat(save_one_obj)])\n",
    "    gt_sample_lens = lens\n",
    "    new_dict = {}\n",
    "    new_dict['final_hum'] = final_hum\n",
    "    new_dict['final_obj'] = final_obj\n",
    "    new_dict['gt_sample_lens'] = gt_sample_lens\n",
    "    save_all_list.append(new_dict)\n",
    "#     topk_lens = min(6, len(sim_))\n",
    "#     topk_sample = sim_.topk(topk_lens,dim=-1)[1][:lens] \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     # ref_x, ref_y = torch.nonzero(topk_sample != indexes).unbind(1)\n",
    "#     # final_indexes = topk_sample[ref_x,ref_y].reshape(-1,topk_lens-1)\n",
    "#     # ref_hum_embs = new_hum[final_indexes].unsqueeze(1).repeat(1,topk_lens-1,1,1)\n",
    "#     ref_hum_embs = new_hum[topk_sample[:,1:]].unsqueeze(1).repeat(1,topk_lens-1,1,1)\n",
    "#     print(\"original lens: {}, augmented lens:{}, label:{}\".format(lens,ref_hum_embs.shape[1]*ref_hum_embs.shape[0], hico_verb_object_list[i]))\n",
    "#     pdb.set_trace()\n",
    "\n",
    "#     object_cls = self.HOI_IDX_TO_OBJ_IDX[i]\n",
    "#     object_ref_embs = torch.stack([torch.as_tensor(obj) for obj in object_feat[object_cls]])\n",
    "    \n",
    "#     sim_obj = obj_emb @ object_ref_embs.t()\n",
    "#     topk_sample_obj = sim_obj.topk(topk_lens-1,dim=-1)[1][:lens]\n",
    "#     ref_obj_embs = object_ref_embs[topk_sample_obj].unsqueeze(2).repeat(1,1,topk_lens-1,1)\n",
    "\n",
    "#     hum_emb = torch.cat([hum_emb,ref_hum_embs.view(-1,512)],dim=0)\n",
    "#     obj_emb = torch.cat([obj_emb,ref_obj_embs.view(-1,512)],dim=0)\n",
    "\n",
    "#     new_lens = len(hum_emb)\n",
    "\n",
    "#     new_K_shot = min(new_lens, K_shot)\n",
    "#     sample_index = np.random.choice(new_lens,new_K_shot,replace=False)\n",
    "\n",
    "#     sample_hum_embeddings =  hum_emb[sample_index]\n",
    "#     sample_obj_embeddings =  obj_emb[sample_index]\n",
    "#     save_embs = torch.cat([hum_emb,obj_emb],dim=-1)\n",
    "#     # pdb.set_trace()\n",
    "#     if i in self.unseen_rare_first and len(save_embs)>=2:\n",
    "#         # pdb.set_trace()\n",
    "#         save_dir = 'save_visual/{}_{}.png'.format('_'.join(self.hico_verb_object_list[i]),lens)\n",
    "#         self.draw_pic(save_embs, lens, save_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "281f3ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([    7,    31,    77,    92,   102,   102,   102,   121,   146,   177,\n",
       "          185,   186,   200,   213,   215,   262,   272,   275,   287,   332,\n",
       "          408,   417,   419,   429,   441,   448,   470,   504,   509,   517,\n",
       "          529,   529,   562,   572,   606,   706,   717,   735,   800,   868,\n",
       "          974,   988,  1057,  1086,  1131,  1178,  1207,  1224,  1236,  1238,\n",
       "         1277,  1337,  1358,  1386,  1396,  1447,  1451,  1479,  1642,  1665,\n",
       "         1675,  1689,  1811,  1838,  1863,  2228,  2345,  2352,  2415,  2424,\n",
       "         2622,  2949,  3587,  4396,  4889,  5594,  5787,  7249,  9462, 10369]),\n",
       "indices=tensor([70, 68, 11, 22, 72, 71, 78, 50, 64, 45, 21, 49, 61, 69, 75, 58, 65, 26,\n",
       "        10, 12, 35, 66, 42, 47, 51, 48, 15, 44, 76, 59, 62, 77, 38, 79, 74, 53,\n",
       "        23, 52,  9, 54, 14, 40, 41,  7, 19, 37, 29, 57, 27, 55, 67, 63, 46, 16,\n",
       "         6, 28, 43, 39, 20,  4, 18, 32,  0, 56, 34, 24, 33, 13,  2, 73, 31, 30,\n",
       "         5, 25, 17, 36, 60,  3,  1,  8]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value, index = torch.as_tensor([len(i) for i in object_feat]).sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "46f4388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hico_obj_text_label = [(0, 'a photo of a person'), (1, 'a photo of a bicycle'), (2, 'a photo of a car'),\n",
    "                       (3, 'a photo of a motorcycle'), (4, 'a photo of an airplane'), (5, 'a photo of a bus'),\n",
    "                       (6, 'a photo of a train'), (7, 'a photo of a truck'), (8, 'a photo of a boat'),\n",
    "                       (9, 'a photo of a traffic light'), (10, 'a photo of a fire hydrant'),\n",
    "                       (11, 'a photo of a stop sign'), (12, 'a photo of a parking meter'), (13, 'a photo of a bench'),\n",
    "                       (14, 'a photo of a bird'), (15, 'a photo of a cat'), (16, 'a photo of a dog'),\n",
    "                       (17, 'a photo of a horse'), (18, 'a photo of a sheep'), (19, 'a photo of a cow'),\n",
    "                       (20, 'a photo of an elephant'), (21, 'a photo of a bear'), (22, 'a photo of a zebra'),\n",
    "                       (23, 'a photo of a giraffe'), (24, 'a photo of a backpack'), (25, 'a photo of a umbrella'),\n",
    "                       (26, 'a photo of a handbag'), (27, 'a photo of a tie'), (28, 'a photo of a suitcase'),\n",
    "                       (29, 'a photo of a frisbee'), (30, 'a photo of a skis'), (31, 'a photo of a snowboard'),\n",
    "                       (32, 'a photo of a sports ball'), (33, 'a photo of a kite'), (34, 'a photo of a baseball bat'),\n",
    "                       (35, 'a photo of a baseball glove'), (36, 'a photo of a skateboard'),\n",
    "                       (37, 'a photo of a surfboard'), (38, 'a photo of a tennis racket'), (39, 'a photo of a bottle'),\n",
    "                       (40, 'a photo of a wine glass'), (41, 'a photo of a cup'), (42, 'a photo of a fork'),\n",
    "                       (43, 'a photo of a knife'), (44, 'a photo of a spoon'), (45, 'a photo of a bowl'),\n",
    "                       (46, 'a photo of a banana'), (47, 'a photo of an apple'), (48, 'a photo of a sandwich'),\n",
    "                       (49, 'a photo of an orange'), (50, 'a photo of a broccoli'), (51, 'a photo of a carrot'),\n",
    "                       (52, 'a photo of a hot dog'), (53, 'a photo of a pizza'), (54, 'a photo of a donut'),\n",
    "                       (55, 'a photo of a cake'), (56, 'a photo of a chair'), (57, 'a photo of a couch'),\n",
    "                       (58, 'a photo of a potted plant'), (59, 'a photo of a bed'), (60, 'a photo of a dining table'),\n",
    "                       (61, 'a photo of a toilet'), (62, 'a photo of a tv'), (63, 'a photo of a laptop'),\n",
    "                       (64, 'a photo of a mouse'), (65, 'a photo of a remote'), (66, 'a photo of a keyboard'),\n",
    "                       (67, 'a photo of a cell phone'), (68, 'a photo of a microwave'), (69, 'a photo of an oven'),\n",
    "                       (70, 'a photo of a toaster'), (71, 'a photo of a sink'), (72, 'a photo of a refrigerator'),\n",
    "                       (73, 'a photo of a book'), (74, 'a photo of a clock'), (75, 'a photo of a vase'),\n",
    "                       (76, 'a photo of a scissors'), (77, 'a photo of a teddy bear'), (78, 'a photo of a hair drier'),\n",
    "                       (79, 'a photo of a toothbrush'), (80, 'a photo of nothing')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "10e7ba4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('direct', 'car')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hico_verb_object_list[66]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f52039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qpic",
   "language": "python",
   "name": "qpic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
